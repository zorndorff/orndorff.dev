+++
title = "Rethinking AI in Education: Veritasium and Cognitive Science"
date = "2025-04-15"
categories = ["Engineering"]
tags = ["AI", "Education", "Cognitive Science"]
type = "posts"
draft = false
+++

> My Notes after watching Derek Muller's talk on AI in education.

<iframe width="560" height="315" src="https://www.youtube.com/embed/0xS68sl2D70?si=n1B0iyg7b0r-NBf7" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

As someone working in EdTech, especially focusing on what we can do with LLMs, you hear it constantly: AI is going to "revolutionize" education. Personalized tutors, instant feedback, access like never before – the whole pitch. But I caught a talk recently by Derek Muller – you know, the Veritasium guy – and he offered this really grounded take that hit home for me. It’s making me rethink the hype and actually consider the cognitive science underneath it all.

Muller started off acknowledging, yeah, AI tutors are impressive. He even pointed to an example from a year ago that still holds up. But right away, he started poking holes in that "revolution" narrative. He did a masterful job reminding us this isn't the first time we've heard this song. Film, radio, TV, computers, MOOCs – every single one was supposed to be the thing that fundamentally replaces teachers and textbooks. And yet, well, here we are.

So why do these revolutions consistently fizzle out? Muller makes the case, and I think he's right, that the technology often tries to solve the wrong problem. The bottleneck isn't always _access_ to information; the real challenge is the _process_ of learning itself.

He leaned heavily on Daniel Kahneman's "Thinking, Fast and Slow," framing learning with System 1 (the fast, intuitive, automatic stuff) and System 2 (the slow, effortful, analytical engine). Real, durable learning – the kind that actually builds expertise and lets System 1 eventually run smoothly – absolutely requires firing up that limited, energy-hungry System 2. It means effortful practice, managing your cognitive load, and piece by piece, building complex knowledge structures – chunking, basically – into long-term memory.

And this is exactly where Muller sees both the potential and the major peril of AI in education:

1.  **The Potential (Where AI _Could_ Support System 2):** Okay, AI _can_ be a solid tool here. It can give you that immediate, specific feedback that’s critical when you're trying to nail down a skill (like the AI tutor correcting the student on trigonometry in his example). It can provide scaffolding, break down complex problems, offer hints – essentially, it can help manage the intrinsic cognitive load and give the learner support _during_ that tough System 2 processing. This lines up pretty well with the goals many of us have when building these AI learning apps.

2.  **The Peril (Where AI Bypasses System 2):** But here’s Muller's biggest concern, and it’s one I share: AI's potential to just _eliminate_ the need for that critical, effortful practice. If an LLM can just write the essay for you, solve the coding problem, generate the whole analysis, then the student might completely bypass the cognitive struggle – the mental 'lifting' – that’s required to actually internalize the concepts and build those vital long-term memory structures. Without putting in the reps, you’re just not going to develop true mastery or understanding. It’s unlikely.

Muller wrapped up by pointing out that education is fundamentally social and human. Teachers act as motivators, they provide accountability, they build community – sort of like a personal trainer who pushes you through the hard work when you want to quit. Information is everywhere, it's abundant; the real difficulty is fostering the motivation and facilitating the _effort_ needed to actually learn.

For engineers like us working in education technology, this is a crucial takeaway. Our goal shouldn't be simply to make education _easier_ by offloading the cognitive work onto the AI. That feels like targeting the wrong thing. Instead, we need to architect tools – leveraging LLMs and whatever else – that skillfully _support_ the effortful learning process. How can AI provide better scaffolding, more insightful feedback, adaptive practice that actually encourages mastery, maybe even foster curiosity, without just becoming a crutch that ultimately gets in the way of deep learning?

Muller's talk was a potent reminder. While AI gives us exciting new capabilities for the toolkit, understanding the cognitive foundations of learning is paramount if we want to build technology that genuinely enhances education, instead of just promising yet another revolution that doesn't quite land. Just some thoughts on why maybe we need to be more deliberate in how we apply these powerful tools.
