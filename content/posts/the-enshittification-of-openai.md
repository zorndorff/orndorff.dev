+++
date = "2025-08-08"
title = "That terrible presentation, the enshittification of OpenAi"
categories = ["engineering", "software", "ai"]
tags = ["engineering", "software", "ai", "rambling"]
type = "posts"
draft = false
+++

Thinking about the GPT-5 presentation fiasco yesterday(friends don't let friends use Dalle for charts) and the resulting, almost overwhelmingly negative reaction to the style of the speakers and the substance. I'm wondering if what we're seeing is less a problem with LLMs having hit a 'wall' and more with the 'enshittification' of OpenAI itself?

They've never been particularly strong on the pure research side of things. Their main strength has always been productizing scientific breakthroughs in consumer products. Take the fundamental 'attention is all you need' paper and transformers architecture. Neither of those were OpenAI breakthroughs.
Instead, their incredibly talented early team identified ways to capitalize on those important insights with their own breakthroughs in model training and scaling.  

They've been masters of commercializing the training and deployment of these models, but I'm increasingly skeptical they have the scientific proficiency internally after years of high-level departures to successfully push forward the state of the art. 
The brain drain was bound to have an effect eventually and GPT-5 may be the harbinger of OpenAI having 'found its level' as a frontier research lab. 
Looking at derpartures over the last 2 years, it's a who's who of the company's foundational talent. Consider the key figures who have walked away in just the last couple of years:

*   **Ilya Sutskever**, a co-founder and the company's Chief Scientist who was central to many of OpenAI's breakthroughs, left in May 2024 to start a new venture focused purely on AI safety, Safe Superintelligence (SSI).
*   **Jan Leike**, who co-led the "Superalignment" team with Sutskever, resigned shortly after, famously stating that "safety culture and processes have taken a backseat to shiny products". He immediately joined rival Anthropic.
*   **John Schulman**, another co-founder and a key mind behind the training of ChatGPT, also departed for Anthropic in August 2024 to get back to "hands-on technical work" on AI alignment.

The C-suite was hollowed out in late 2024 with the near-simultaneous resignations of CTO **Mira Murati** (who went on to found Thinking Machines Lab), Chief Research Officer **Bob McGrew**, and VP of Research **Barret Zoph**. Even the product leadership has seen churn, with **Peter Deng**, the VP of Consumer Product who led ChatGPT's rollout, leaving to become a venture capitalist at Felicis Ventures.

Meta has aggressively poached top researchers with massive compensation packages, luring away core model builders like **Shengjia Zhao** (a co-creator of ChatGPT, now Chief Scientist at Meta's new lab), **Jason Wei**, and **Hyung Won Chung** (key contributors to the `o1` and `o3` reasoning models), and entire teams working on foundational models like GPT-4.

GPT-5 appears to have 'found the ceiling' for just increasing the amount of training data and hoping for the best.

Labs like Anthropic, Deepmind, and new founder-led startups are pushing the frontier forward at the architectural level by doing increasingly more with less.
They're doing it with the very talent that has left OpenAI in droves over the last two years for mostly ideological reasons pointing towards this exact issue. Anthropic, founded by former OpenAI VP of Research Dario Amodei over similar concerns years ago, has become a haven for those who feel OpenAI has lost its way, now employing key figures like **Jan Leike** and co-founder **John Schulman**. Meanwhile, the "OpenAI Diaspora" has spawned formidable new competitors from scratch, including **Ilya Sutskever's** Safe Superintelligence and **Mira Murati's** Thinking Machines Lab, both of which are now attracting top talent and significant funding.

The current iteration of OpenAI, in my opinion, is suffering from the classic symptoms of enshitification. They're creating new features at the expense of real utility, spending vast sums of money for incremental gains, and losing developer trust in the process.
Personally, I don't really care what's going on in OpenAI since there's so much competition in the space, their models will likely remain some of the best bang-for-the-buck, even if they increasingly lag behind the likes of Anthropic and Google, not to mention the many Chinese labs that are kicking tail when it comes to raw research progress.
